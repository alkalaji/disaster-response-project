{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "def load_dataset(path, index_col):\n",
    "    '''\n",
    "    Loads a dataset from a CSV file\n",
    "\n",
    "            Parameters:\n",
    "                    path(str): CSV file path\n",
    "                    index_col (str): name of column to use as index\n",
    "\n",
    "            Returns:\n",
    "                    df (pd.DataFrame): DataFrame of the dataset\n",
    "    '''\n",
    "    df = None\n",
    "    try:\n",
    "        df = pd.read_csv(path, index_col=index_col)\n",
    "    except:\n",
    "        print('Error while reading dataset: ' + path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_categories(categories: pd.DataFrame):\n",
    "    '''\n",
    "    Clean categories dataset columns and values\n",
    "\n",
    "            Parameters:\n",
    "                    categories(pd.DataFrame): CSV file path\n",
    "\n",
    "            Returns:\n",
    "                    categories (pd.DataFrame): DataFrame of the dataset\n",
    "    '''\n",
    "    # create a dataframe of the 36 individual category columns\n",
    "    categories = categories.categories.str.split(pat=';', expand=True)\n",
    "\n",
    "    # select the first row of the categories dataframe\n",
    "    row = categories.iloc[0]\n",
    "\n",
    "    # Removing the numerical part of the column name using str split\n",
    "    category_colnames = list(row.str.split('-').str[0])\n",
    "\n",
    "    # rename the columns of `categories`\n",
    "    categories.columns = category_colnames\n",
    "\n",
    "    # Convert category values to numeric for the newly created columns\n",
    "    for column in categories:\n",
    "        # set each value to be the last character of the string\n",
    "        categories[column] = categories[column].str.split('-').str[1]\n",
    "\n",
    "        # convert column from string to numeric\n",
    "        categories[column] = pd.to_numeric(categories[column])\n",
    "    \n",
    "    # Remove rows that have value 2 in related column\n",
    "    # There are 193 valuess. which is insignificant if we drop, \n",
    "    # given that they don't hold a special meaning\n",
    "    categories = categories.drop(index=categories[categories.related == 2].index)\n",
    "    \n",
    "    return categories\n",
    "\n",
    "\n",
    "def create_dataframe(messages, categories):\n",
    "    '''\n",
    "    Create full dataset combining messages and categories\n",
    "\n",
    "            Parameters:\n",
    "                    messages(pd.DataFrame): the messages dataset\n",
    "                    categories(pd.DataFrame): the categories dataset\n",
    "\n",
    "            Returns:\n",
    "                    df (pd.DataFrame): DataFrame containing the full dataset\n",
    "    '''\n",
    "    # merge datasets\n",
    "    df = pd.merge(messages, categories, how='inner', left_on='id', right_on='id')\n",
    "\n",
    "    print('Removing duplicates')\n",
    "    # check number of duplicates\n",
    "    dups = df.shape[0] - df.drop_duplicates().shape[0]\n",
    "    if dups > 0:\n",
    "        # drop duplicates\n",
    "        df = df.drop_duplicates()\n",
    "        print('Removed {} duplicates'.format(dups))\n",
    "    else:\n",
    "        print('No duplicates found!')\n",
    "    return df\n",
    "\n",
    "\n",
    "def write_to_db(df: pd.DataFrame, db_name):\n",
    "    '''\n",
    "    Write dataframe to database\n",
    "\n",
    "            Parameters:\n",
    "                    df(pd.DataFrame): the dataframe to be written to db\n",
    "                    db_name(str): name of the database\n",
    "    '''\n",
    "    try:\n",
    "        engine = create_engine('sqlite:///' + db_name)\n",
    "        df.to_sql('messages_and_categories', engine, index=False, if_exists='replace')\n",
    "    except:\n",
    "        print('Failed to write to database')\n",
    "\n",
    "\n",
    "def main():\n",
    "    # default paths in case no arguments were provided\n",
    "    messages_path = './data/messages.csv'\n",
    "    categories_path = './data/categories.csv'\n",
    "    database_name = './data/DisasterResponse.db'\n",
    "\n",
    "    if len(sys.argv) == 4:\n",
    "        messages_path = sys.argv[1]\n",
    "        categories_path = sys.argv[2]\n",
    "        database_name = sys.argv[3]\n",
    "\n",
    "    print('Starting ETL pipeline')\n",
    "\n",
    "    # load messages dataset\n",
    "    print('Loading \"{}\"'.format(messages_path))\n",
    "    messages = load_dataset(messages_path, 'id')\n",
    "    # load categories dataset\n",
    "    print('Loading \"{}\"'.format((categories_path)))\n",
    "    categories = load_dataset(categories_path, 'id')\n",
    "\n",
    "    # clean categories\n",
    "    print('Cleaning categories')\n",
    "    categories = clean_categories(categories)\n",
    "\n",
    "    # create dataset dataframe\n",
    "    print('Creating dataset dataframe')\n",
    "    df = create_dataframe(messages, categories)\n",
    "\n",
    "    # write to database\n",
    "    print('Writing to database')\n",
    "    write_to_db(df, database_name)\n",
    "\n",
    "    print('Finished ETL pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ETL pipeline\n",
      "Loading \"./data/messages.csv\"\n",
      "Loading \"./data/categories.csv\"\n",
      "Cleaning categories\n",
      "Creating dataset dataframe\n",
      "Removing duplicates\n",
      "Removed 154 duplicates\n",
      "Writing to database\n",
      "Finished ETL pipeline\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alien\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alien\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alien\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ML pipeline\n",
      "Loading data from database\n",
      "Fitting model\n",
      "Using GridSearchCV\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5 ...........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, score=0.638, total=  27.0s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5 ...........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   26.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, score=0.647, total=  27.5s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5 ...........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   54.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, score=0.638, total=  27.6s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5 ...........................\n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, score=0.640, total=  27.7s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5 ...........................\n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, score=0.637, total=  27.7s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75 ..........................\n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, score=0.632, total=  27.6s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75 ..........................\n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, score=0.640, total=  27.7s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75 ..........................\n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, score=0.640, total=  27.7s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75 ..........................\n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, score=0.635, total=  27.8s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75 ..........................\n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, score=0.635, total=  27.8s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0 ...........................\n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, score=0.636, total=  29.5s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0 ...........................\n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, score=0.646, total=  27.7s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0 ...........................\n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, score=0.632, total=  27.7s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0 ...........................\n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, score=0.639, total=  27.8s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0 ...........................\n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, score=0.632, total=  27.6s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5 ..........................\n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, score=0.634, total=  28.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5 ..........................\n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, score=0.647, total=  28.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5 ..........................\n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, score=0.635, total=  28.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5 ..........................\n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, score=0.643, total=  28.3s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5 ..........................\n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, score=0.638, total=  28.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75 .........................\n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, score=0.635, total=  28.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75 .........................\n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, score=0.641, total=  28.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75 .........................\n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, score=0.632, total=  28.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75 .........................\n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, score=0.637, total=  28.3s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75 .........................\n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, score=0.632, total=  28.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0 ..........................\n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, score=0.629, total=  28.0s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0 ..........................\n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, score=0.639, total=  28.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0 ..........................\n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, score=0.635, total=  32.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0 ..........................\n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, score=0.639, total=  28.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0 ..........................\n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, score=0.633, total=  27.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 14.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions\n",
      "Model scores per class:\n",
      "Label name:  related\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.8192879098360656\n",
      "F1 score:  0.8894806924101198\n",
      "Precision: 0.8309673642616713\n",
      "recall: 0.9568587799123695\n",
      "-----------------------\n",
      "Label name:  request\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.897797131147541\n",
      "F1 score:  0.6246472248353716\n",
      "Precision: 0.8258706467661692\n",
      "recall: 0.5022692889561271\n",
      "-----------------------\n",
      "Label name:  offer\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9966700819672131\n",
      "F1 score:  0.0\n",
      "Precision: 0.0\n",
      "recall: 0.0\n",
      "-----------------------\n",
      "Label name:  aid_related\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.7784323770491803\n",
      "F1 score:  0.7215963952365625\n",
      "Precision: 0.7460898502495841\n",
      "recall: 0.6986600186974136\n",
      "-----------------------\n",
      "Label name:  medical_help\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9216188524590164\n",
      "F1 score:  0.10263929618768328\n",
      "Precision: 0.6363636363636364\n",
      "recall: 0.05582137161084529\n",
      "-----------------------\n",
      "Label name:  medical_products\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.953125\n",
      "F1 score:  0.12857142857142856\n",
      "Precision: 0.8181818181818182\n",
      "recall: 0.06976744186046512\n",
      "-----------------------\n",
      "Label name:  search_and_rescue\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9728483606557377\n",
      "F1 score:  0.12396694214876033\n",
      "Precision: 0.7894736842105263\n",
      "recall: 0.06726457399103139\n",
      "-----------------------\n",
      "Label name:  security\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9806608606557377\n",
      "F1 score:  0.013071895424836602\n",
      "Precision: 1.0\n",
      "recall: 0.006578947368421052\n",
      "-----------------------\n",
      "Label name:  military\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9691342213114754\n",
      "F1 score:  0.11070110701107012\n",
      "Precision: 0.75\n",
      "recall: 0.05976095617529881\n",
      "-----------------------\n",
      "Label name:  child_alone\n",
      "Label values: [0]\n",
      "Accuracy:  1.0\n",
      "F1 score:  0.0\n",
      "Precision: 0.0\n",
      "recall: 0.0\n",
      "-----------------------\n",
      "Label name:  water\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9581198770491803\n",
      "F1 score:  0.5226277372262773\n",
      "Precision: 0.9040404040404041\n",
      "recall: 0.3675564681724846\n",
      "-----------------------\n",
      "Label name:  food\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9383965163934426\n",
      "F1 score:  0.6752194463200539\n",
      "Precision: 0.8278145695364238\n",
      "recall: 0.5701254275940707\n",
      "-----------------------\n",
      "Label name:  shelter\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9394211065573771\n",
      "F1 score:  0.5490943755958055\n",
      "Precision: 0.8228571428571428\n",
      "recall: 0.41201716738197425\n",
      "-----------------------\n",
      "Label name:  clothing\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9852715163934426\n",
      "F1 score:  0.217687074829932\n",
      "Precision: 0.7272727272727273\n",
      "recall: 0.128\n",
      "-----------------------\n",
      "Label name:  money\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9766905737704918\n",
      "F1 score:  0.052083333333333336\n",
      "Precision: 0.7142857142857143\n",
      "recall: 0.02702702702702703\n",
      "-----------------------\n",
      "Label name:  missing_people\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9883452868852459\n",
      "F1 score:  0.0\n",
      "Precision: 0.0\n",
      "recall: 0.0\n",
      "-----------------------\n",
      "Label name:  refugees\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9699026639344263\n",
      "F1 score:  0.06374501992031872\n",
      "Precision: 0.6153846153846154\n",
      "recall: 0.03361344537815126\n",
      "-----------------------\n",
      "Label name:  death\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9569672131147541\n",
      "F1 score:  0.1923076923076923\n",
      "Precision: 0.8333333333333334\n",
      "recall: 0.10869565217391304\n",
      "-----------------------\n",
      "Label name:  other_aid\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.8717981557377049\n",
      "F1 score:  0.04393505253104107\n",
      "Precision: 0.6388888888888888\n",
      "recall: 0.02274975272007913\n",
      "-----------------------\n",
      "Label name:  infrastructure_related\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9336577868852459\n",
      "F1 score:  0.0\n",
      "Precision: 0.0\n",
      "recall: 0.0\n",
      "-----------------------\n",
      "Label name:  transport\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9600409836065574\n",
      "F1 score:  0.1702127659574468\n",
      "Precision: 0.7804878048780488\n",
      "recall: 0.0955223880597015\n",
      "-----------------------\n",
      "Label name:  buildings\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9515881147540983\n",
      "F1 score:  0.19230769230769232\n",
      "Precision: 0.9183673469387755\n",
      "recall: 0.10739856801909307\n",
      "-----------------------\n",
      "Label name:  electricity\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9816854508196722\n",
      "F1 score:  0.07741935483870968\n",
      "Precision: 1.0\n",
      "recall: 0.040268456375838924\n",
      "-----------------------\n",
      "Label name:  tools\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9935963114754098\n",
      "F1 score:  0.0\n",
      "Precision: 0.0\n",
      "recall: 0.0\n",
      "-----------------------\n",
      "Label name:  hospitals\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9879610655737705\n",
      "F1 score:  0.0\n",
      "Precision: 0.0\n",
      "recall: 0.0\n",
      "-----------------------\n",
      "Label name:  shops\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9956454918032787\n",
      "F1 score:  0.0\n",
      "Precision: 0.0\n",
      "recall: 0.0\n",
      "-----------------------\n",
      "Label name:  aid_centers\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9891137295081968\n",
      "F1 score:  0.0\n",
      "Precision: 0.0\n",
      "recall: 0.0\n",
      "-----------------------\n",
      "Label name:  other_infrastructure\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9559426229508197\n",
      "F1 score:  0.0\n",
      "Precision: 0.0\n",
      "recall: 0.0\n",
      "-----------------------\n",
      "Label name:  weather_related\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.8807633196721312\n",
      "F1 score:  0.7613432453217125\n",
      "Precision: 0.8418367346938775\n",
      "recall: 0.6948993916705662\n",
      "-----------------------\n",
      "Label name:  floods\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9550461065573771\n",
      "F1 score:  0.6277836691410392\n",
      "Precision: 0.9024390243902439\n",
      "recall: 0.4813008130081301\n",
      "-----------------------\n",
      "Label name:  storm\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9394211065573771\n",
      "F1 score:  0.5897658282740676\n",
      "Precision: 0.7657657657657657\n",
      "recall: 0.4795486600846262\n",
      "-----------------------\n",
      "Label name:  fire\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9897540983606558\n",
      "F1 score:  0.0\n",
      "Precision: 0.0\n",
      "recall: 0.0\n",
      "-----------------------\n",
      "Label name:  earthquake\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.969390368852459\n",
      "F1 score:  0.8266860043509789\n",
      "Precision: 0.9018987341772152\n",
      "recall: 0.7630522088353414\n",
      "-----------------------\n",
      "Label name:  cold\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9811731557377049\n",
      "F1 score:  0.1787709497206704\n",
      "Precision: 0.9411764705882353\n",
      "recall: 0.09876543209876543\n",
      "-----------------------\n",
      "Label name:  other_weather\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.9485143442622951\n",
      "F1 score:  0.04739336492890995\n",
      "Precision: 0.7142857142857143\n",
      "recall: 0.024509803921568627\n",
      "-----------------------\n",
      "Label name:  direct_report\n",
      "Label values: [0 1]\n",
      "Accuracy:  0.8627049180327869\n",
      "F1 score:  0.505078485687904\n",
      "Precision: 0.8008784773060029\n",
      "recall: 0.36884693189480783\n",
      "-----------------------\n",
      "\n",
      "Best Model Parameters: {'tfidf__use_idf': True, 'vect__max_df': 0.5}\n",
      "Saving model\n",
      "Finished ML pipeline\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "# Load data from database\n",
    "def load_from_db(database_name):\n",
    "    '''\n",
    "    Load the dataset generated from ETL pipeline\n",
    "\n",
    "            Returns:\n",
    "                    X (pd.DataFrame): DataFrame containing the dataset\n",
    "                    y (pd.DataFrame): Labels of the data\n",
    "    '''\n",
    "    engine = create_engine('sqlite:///' + database_name)\n",
    "    df = pd.read_sql('messages_and_categories', engine)\n",
    "    X = df.message\n",
    "    y = df.drop(['message', 'original', 'genre'], axis=1)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    '''\n",
    "    Tokenize function performs multiple preprocessing steps:\n",
    "        - Converts text to lower case\n",
    "        - Replaces any special character\n",
    "        - Tokenizes text\n",
    "        - Removes stop words\n",
    "        - Lemmatize\n",
    "        - Stem\n",
    "\n",
    "            Parameters:\n",
    "                    text (str): a message to be tokenized\n",
    "\n",
    "            Returns:\n",
    "                    text (str): Preprocessed and tokenized text\n",
    "    '''\n",
    "    # Normalize\n",
    "    text = text.lower()\n",
    "    text = text.replace(r\"([^a-zA-Z0-9])\", ' ')\n",
    "\n",
    "    # Tokenize\n",
    "    text = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words, stem and lemmatize.\n",
    "    # Those were combined in order not to iterate multiple times\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = [stemmer.stem(lemmatizer.lemmatize(w.strip()))\n",
    "            for w in text if w not in stopwords_list]\n",
    "    return text\n",
    "\n",
    "\n",
    "# Display class-level performance metrics for the best model\n",
    "def display_results(cv, y_test, y_pred):\n",
    "    '''\n",
    "    Display the model metrics; Accuracy, F1 score, Precision, Recall\n",
    "    In case the pipeline is the result of some search algorithm the best_params are displayed.\n",
    "\n",
    "            Parameters:\n",
    "                    cv (Pipeline): The pipeline containing the model\n",
    "                    y_test (numpy.array): Ground truth labels for test data\n",
    "                    y_pred (numpy.array): Model output predictions\n",
    "    '''\n",
    "\n",
    "    for i in range(y_test.shape[1]):\n",
    "        col = y_test.iloc[:, i]\n",
    "\n",
    "        print('Label name: ', col.name)\n",
    "        print('Label values:', np.unique(col))\n",
    "        print('Accuracy: ', accuracy_score(col, y_pred[:, i]))\n",
    "        print('F1 score: ', f1_score(col, y_pred[:, i], zero_division=0))\n",
    "        print('Precision:', precision_score(col, y_pred[:, i], zero_division=0))\n",
    "        print('recall:', recall_score(col, y_pred[:, i], zero_division=0))\n",
    "        print('-----------------------')\n",
    "\n",
    "    # This is in case what was being passed is just a model not as a result of gridserch\n",
    "    if hasattr(cv, 'best_params_'):\n",
    "        print(\"\\nBest Model Parameters:\", cv.best_params_)\n",
    "\n",
    "\n",
    "def build_model(search_method=None):\n",
    "    '''\n",
    "    Build ml pipeline. In case a search method was specified it will be used to fit the pipeline\n",
    "\n",
    "            Parameters:\n",
    "                    search_method (str): The search method to be used if any (default: None)\n",
    "                        'grid': GridSearchCV\n",
    "                        'randomized': RandomizedSearchCV\n",
    "                        None: no search method is to be used\n",
    "\n",
    "            Returns:\n",
    "                    cv (Pipeline): The pipeline containing the model\n",
    "    '''\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier(n_jobs=-1), n_jobs=-1)),\n",
    "    ])\n",
    "\n",
    "    # The parameters were commented out because the execution time was very long on my machine\n",
    "    # Only kept one to showcase that the implementation works\n",
    "    parameters = {\n",
    "        # 'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "        'vect__max_df': (0.5, 0.75, 1.0),\n",
    "        # 'vect__max_features': (None, 5000, 10000),\n",
    "        'tfidf__use_idf': (True, False)\n",
    "    }\n",
    "\n",
    "    if search_method == 'grid':\n",
    "        print('Using GridSearchCV')\n",
    "        cv = GridSearchCV(pipeline, param_grid=parameters, scoring='f1_micro', verbose=3)\n",
    "    elif search_method == 'randomized':\n",
    "        print('Using RandomizedSearchCV')\n",
    "        cv = RandomizedSearchCV(pipeline, param_distributions=parameters, verbose=2)\n",
    "    else:\n",
    "        print('Using the pipeline without a search method')\n",
    "        # if no grid search is needed, just return the pipeline\n",
    "        cv = pipeline\n",
    "\n",
    "    return cv\n",
    "\n",
    "\n",
    "# Save model to file\n",
    "def save_model(model, path):\n",
    "    '''\n",
    "    Save specified model to path\n",
    "\n",
    "            Parameters:\n",
    "                    model (Pipeline): The pipeline containing the model\n",
    "                    path (str): path to write the pickle file\n",
    "\n",
    "    '''\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # default paths in case no arguments were provided\n",
    "    database_name = './data/DisasterResponse.db'\n",
    "    model_file_path = './best_model.pkl'\n",
    "\n",
    "\n",
    "    if len(sys.argv) == 4:\n",
    "        database_name = sys.argv[1]\n",
    "        model_file_path = sys.argv[2]\n",
    "\n",
    "    print('Starting ML pipeline')\n",
    "\n",
    "    # Load data from database\n",
    "    print('Loading data from database')\n",
    "    X, y = load_from_db(database_name)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # build classifier\n",
    "    print('Fitting model')\n",
    "    model = build_model(search_method='grid')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # predict on test data\n",
    "    print('Generating predictions')\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # display results\n",
    "    print('Model scores per class:')\n",
    "    display_results(model, y_test, y_pred)\n",
    "\n",
    "    # Get the best model, in case GridSearch/RandomizedSearch was used. Which is the best_estimator_.\n",
    "    # Otherwise, it is just the model itself\n",
    "    best_model = model\n",
    "    if hasattr(model, 'best_estimator_'):\n",
    "        best_model = model.best_estimator_\n",
    "\n",
    "    # Save model\n",
    "    print('Saving model')\n",
    "    save_model(best_model, model_file_path)\n",
    "\n",
    "    print('Finished ML pipeline')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
